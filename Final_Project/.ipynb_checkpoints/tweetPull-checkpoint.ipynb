{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUNY Data 620 - Web Analytics, Summer 2020  \n",
    "**Final Project: Twitter Pull**   \n",
    "**Prof:** Alain Ledon  \n",
    "**Members:** Misha Kollontai, Amber Ferger, Zach Alexander, Subhalaxmi Rout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import GetOldTweets3 as got\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We'll define the following functions:\n",
    "* **perdelta**: Based on a [stackoverflow](https://stackoverflow.com/questions/10688006/generate-a-list-of-datetimes-between-an-interval) thread, this will be used to generate a list of date ranges for our twitter pull. \n",
    "* **getTweets**: This will be used to pull the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ date function\n",
    "def perdelta(start, end, delta):\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta\n",
    "        \n",
    "################ get tweets function \n",
    "def getTweets(city, startDate, endDate):\n",
    "    n = 100\n",
    "    \n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch('COVID')\\\n",
    "    .setSince(startDate)\\\n",
    "    .setUntil(endDate)\\\n",
    "    .setMaxTweets(n)\\\n",
    "    .setNear(city)\n",
    "    \n",
    "    ls = []\n",
    "    \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    print(len(tweets))\n",
    "    for i in tweets:\n",
    "        ls.append([i.text, i.hashtags, i.date, i.username, city, startDate, endDate])\n",
    "    \n",
    "    return(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data\n",
    "\n",
    "#### Largest City by State\n",
    "\n",
    "* Read in a list of the top 1000 [cities]([https://public.opendatasoft.com/explore/dataset/1000-largest-us-cities-by-population-with-geographic-coordinates/table/?sort=-rank]) in the US\n",
    "* Select top city by state, extract geocoordinates\n",
    "* Split the geocoordinates data into 2 lists so that we can run on 2 separate machines (there is a max of 15 requests per 15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cities doc, select top city from each \n",
    "# https://stackoverflow.com/questions/50415632/how-to-select-top-n-row-from-each-group-after-group-by-in-pandas\n",
    "allData = pd.read_csv('largeCities.csv', delimiter=';')\n",
    "final_cities = allData.sort_values(by = ['State', 'Population'], ascending=False).groupby(['State'], sort=False).head(1)\n",
    "coords = final_cities['Coordinates'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the coordinates into 2 lists\n",
    "mid = math.floor(len(coords)/2)\n",
    "coords1 = coords[0:mid]\n",
    "coords2 = coords[mid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Ranges\n",
    "Next, we'll generate date ranges for pull. Each range will represent 2 weeks, defined as Sunday - Saturday. The total span of the analysis will go from **3/8/2020** to **7/11/2020**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = []\n",
    "for result in perdelta(datetime.date(2020, 3, 8), datetime.date(2020, 7, 6), datetime.timedelta(days=7)):  \n",
    "    nextWk = result + datetime.timedelta(days=6)\n",
    "    startDt = result.strftime(\"%Y-%m-%d\")\n",
    "    endDt = nextWk.strftime(\"%Y-%m-%d\")   \n",
    "    all_dates.append((startDt,endDt))\n",
    "    \n",
    "#all_dates = [(datetime.date(2020, 3, 8).strftime(\"%Y-%m-%d\"), datetime.date(2020, 7, 15).strftime(\"%Y-%m-%d\"))]\n",
    "#all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "6\n",
      "30\n",
      "1\n",
      "85\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "86\n",
      "88\n",
      "76\n",
      "87\n",
      "77\n",
      "73\n",
      "57\n",
      "62\n",
      "59\n",
      "64\n",
      "92\n",
      "100\n",
      "2\n",
      "20\n",
      "25\n",
      "31\n",
      "32\n",
      "20\n",
      "16\n",
      "21\n",
      "16\n",
      "16\n",
      "13\n",
      "12\n",
      "7\n",
      "5\n",
      "6\n",
      "11\n",
      "10\n",
      "29\n",
      "100\n",
      "3\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "4\n",
      "41\n",
      "100\n",
      "100\n",
      "100\n",
      "99\n",
      "78\n",
      "61\n",
      "58\n",
      "65\n",
      "76\n",
      "66\n",
      "82\n",
      "47\n",
      "51\n",
      "45\n",
      "60\n",
      "62\n",
      "100\n",
      "5\n",
      "19\n",
      "26\n",
      "21\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured during an HTTP request: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Try to open in browser: https://twitter.com/search?q=COVID%20near%3A%2244.4758825%2C-73.212072%22%20within%3A15mi%20since%3A2020-04-05%20until%3A2020-04-11&src=typd\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\GetOldTweets3\\manager\\TweetManager.py\", line 344, in getJsonResponse\n",
      "    jsonResponse = response.read()\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\http\\client.py\", line 467, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\http\\client.py\", line 608, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-285c7598e2e1>\", line 9, in <module>\n",
      "    ls = getTweets(c,d[0],d[1])\n",
      "  File \"<ipython-input-2-d2afd06644d4>\", line 20, in getTweets\n",
      "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\GetOldTweets3\\manager\\TweetManager.py\", line 65, in getTweets\n",
      "    json = TweetManager.getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, user_agent, debug=debug)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\GetOldTweets3\\manager\\TweetManager.py\", line 348, in getJsonResponse\n",
      "    sys.exit()\n",
      "SystemExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\aferg\\desktop\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cycle through all cities\n",
    "finalList = []\n",
    "\n",
    "for i,c in enumerate(coords1):\n",
    "    print(i)\n",
    "    if (i+1) % 2 == 0:\n",
    "        time.sleep(600) # wait 10 min before continuing\n",
    "    for d in all_dates:\n",
    "        ls = getTweets(c,d[0],d[1])\n",
    "        [finalList.append(x) for x in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(finalList, columns = ['TEXT', 'HASHTAGS', 'TWEET_DATE', 'USERNAME', 'COORDS', 'WEEK_START', 'WEEK_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalCsv = pd.merge(left=df, right=final_cities, left_on='COORDS', right_on='Coordinates')\n",
    "finalCsv = finalCsv.drop(columns=['Rank', 'Growth From 2000 to 2013', 'Coordinates'])\n",
    "finalCsv.to_csv('csvFinal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
