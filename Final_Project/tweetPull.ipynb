{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUNY Data 620 - Web Analytics, Summer 2020  \n",
    "**Final Project: Twitter Pull**   \n",
    "**Prof:** Alain Ledon  \n",
    "**Members:** Misha Kollontai, Amber Ferger, Zach Alexander, Subhalaxmi Rout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import GetOldTweets3 as got\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We'll define the following functions:\n",
    "* **perdelta**: Based on a [stackoverflow](https://stackoverflow.com/questions/10688006/generate-a-list-of-datetimes-between-an-interval) thread, this will be used to generate a list of date ranges for our twitter pull. \n",
    "* **getTweets**: This will be used to pull the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ date function\n",
    "def perdelta(start, end, delta):\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta\n",
    "        \n",
    "################ get tweets function \n",
    "def getTweets(city, startDate, endDate):\n",
    "    n = 100\n",
    "    \n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch('COVID')\\\n",
    "    .setSince(startDate)\\\n",
    "    .setUntil(endDate)\\\n",
    "    .setMaxTweets(n)\\\n",
    "    .setNear(city)\n",
    "    \n",
    "    ls = []\n",
    "    \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    print(len(tweets))\n",
    "    for i in tweets:\n",
    "        ls.append([i.text, i.hashtags, i.date, i.username, city, startDate, endDate])\n",
    "    \n",
    "    return(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data\n",
    "\n",
    "#### Largest City by State\n",
    "\n",
    "* Read in a list of the top 1000 [cities]([https://public.opendatasoft.com/explore/dataset/1000-largest-us-cities-by-population-with-geographic-coordinates/table/?sort=-rank]) in the US\n",
    "* Select top city by state, extract geocoordinates\n",
    "* Split the geocoordinates data into 2 lists so that we can run on 2 separate machines (there is a max of 15 requests per 15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cities doc, select top city from each \n",
    "# https://stackoverflow.com/questions/50415632/how-to-select-top-n-row-from-each-group-after-group-by-in-pandas\n",
    "allData = pd.read_csv('largeCities.csv', delimiter=';')\n",
    "final_cities = allData.sort_values(by = ['State', 'Population'], ascending=False).groupby(['State'], sort=False).head(1)\n",
    "coords = final_cities['Coordinates'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the coordinates into 2 lists\n",
    "mid = math.floor(len(coords)/2)\n",
    "coords1 = coords[0:mid]\n",
    "coords2 = coords[mid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Ranges\n",
    "Next, we'll generate date ranges for pull. Each range will represent 2 weeks, defined as Sunday - Saturday. The total span of the analysis will go from **3/8/2020** to **7/11/2020**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-03-08', '2020-03-14'),\n",
       " ('2020-03-15', '2020-03-21'),\n",
       " ('2020-03-22', '2020-03-28'),\n",
       " ('2020-03-29', '2020-04-04'),\n",
       " ('2020-04-05', '2020-04-11'),\n",
       " ('2020-04-12', '2020-04-18'),\n",
       " ('2020-04-19', '2020-04-25'),\n",
       " ('2020-04-26', '2020-05-02'),\n",
       " ('2020-05-03', '2020-05-09'),\n",
       " ('2020-05-10', '2020-05-16'),\n",
       " ('2020-05-17', '2020-05-23'),\n",
       " ('2020-05-24', '2020-05-30'),\n",
       " ('2020-05-31', '2020-06-06'),\n",
       " ('2020-06-07', '2020-06-13'),\n",
       " ('2020-06-14', '2020-06-20'),\n",
       " ('2020-06-21', '2020-06-27'),\n",
       " ('2020-06-28', '2020-07-04'),\n",
       " ('2020-07-05', '2020-07-11')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dates = []\n",
    "for result in perdelta(datetime.date(2020, 3, 8), datetime.date(2020, 7, 6), datetime.timedelta(days=7)):  \n",
    "    nextWk = result + datetime.timedelta(days=6)\n",
    "    startDt = result.strftime(\"%Y-%m-%d\")\n",
    "    endDt = nextWk.strftime(\"%Y-%m-%d\")   \n",
    "    all_dates.append((startDt,endDt))\n",
    "    \n",
    "#all_dates = [(datetime.date(2020, 3, 8).strftime(\"%Y-%m-%d\"), datetime.date(2020, 7, 15).strftime(\"%Y-%m-%d\"))]\n",
    "all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "6\n",
      "24\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c4754566fc46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# wait 15 min before continuing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_dates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cycle through all cities\n",
    "finalList = []\n",
    "\n",
    "for i,c in enumerate(coords1):\n",
    "    print(i)\n",
    "    if (i+1) % 2 == 0:\n",
    "        time.sleep(900) # wait 15 min before continuing\n",
    "    for d in all_dates:\n",
    "        ls = getTweets(c,d[0],d[1])\n",
    "        [finalList.append(x) for x in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(finalList, columns = ['TEXT', 'HASHTAGS', 'TWEET_DATE', 'USERNAME', 'COORDS', 'WEEK_START', 'WEEK_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalCsv = pd.merge(left=df, right=final_cities, left_on='COORDS', right_on='Coordinates')\n",
    "finalCsv = finalCsv.drop(columns=['Rank', 'Growth From 2000 to 2013', 'Coordinates'])\n",
    "finalCsv.to_csv('csvFinal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
